import pandas as pd
import numpy as np
import torch
import json
import argparse
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig
import transformers
from os.path import join, exists
from os import makedirs
import torch.nn as nn
from transformers.generation import StoppingCriteria, StoppingCriteriaList
import itertools
import os
import itertools
import matplotlib.cm as cm

import pickle
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import umap

# Define the custom stopping criteria
class StopOnAnyTokenSequence(StoppingCriteria):
    def __init__(self, stop_sequences_ids):
        self.stop_sequences_ids = stop_sequences_ids

    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:
        for seq in self.stop_sequences_ids:
            if input_ids.shape[1] >= len(seq):
                if torch.equal(input_ids[0, -len(seq):], torch.tensor(seq, device=input_ids.device)):
                    print('Stop generation', seq)
                    return True
        return False


def set_up_dir(path):
    try:
        os.makedirs(path)
    except OSError:
        if not os.path.isdir(path):
            raise


def get_projection(embeddings, method, labels, ax, c = None, perplexity=50, fontsize=20, s=100,  n_neighbors=15, min_dist=0.1, random_state=42, plot=False): 
    
    # Perform dimensionality reduction
    if method == "pca":
        reducer = PCA(n_components=2, random_state=random_state)
        reduced_embeddings = reducer.fit_transform(embeddings)
        
    elif method == "tsne":
        reducer = TSNE(n_components=2, perplexity=perplexity, random_state=random_state)
        reduced_embeddings = reducer.fit_transform(embeddings)

    elif method == "umap":

        reducer = umap.UMAP(n_components=2, n_neighbors=n_neighbors, min_dist=min_dist, random_state=random_state, metric='euclidean', init="random")
        reduced_embeddings = reducer.fit_transform(embeddings)
    else:
        raise ValueError("Unsupported method. Use 'pca' or 'tsne'.")

    if plot:
        for i, label in enumerate(labels):
            x, y = reduced_embeddings[i]
            #c = color_dict[label]
            ax.scatter(x, y, s=s, alpha=0.7, color= c[i])
            ax.text(x + np.random.normal(0,0.01), y, label, fontsize=fontsize, alpha=0.9)    

    return reduced_embeddings


def get_node_dict(tokenized_chat, node_names):
    
    node_dict = {k: [] for k in node_names}
    for k,v in enumerate(tokenized_chat.detach().squeeze().tolist()):
        token= tokenizer.convert_ids_to_tokens([v])[0]
        if token in node_dict:
            node_dict[token].append(k)

    node_dict_names = {}
    for k,v in node_dict.items():
        for i,v_ in enumerate(v):            
            name = k +' '+ str(i)
            node_dict_names[v_] = k +' '+ str(i)
                
    return node_dict, node_dict_names

def get_token_pair(token_explained):
    if token_explained == 'yes':
        contrast_token = 'no'
    elif  token_explained == 'no':
        contrast_token = 'yes'
    if token_explained == 'Yes':
        contrast_token = 'No'
    elif  token_explained == 'No':
        contrast_token = 'Yes'
    else:
        raise
    return contrast_token

def simplified_forward(layers, norm, rotary_emb, lm_head, inputs_embeds, attention_mask, l_lrp=(None, None)):

    # create position_ids on the fly for batch generation
    position_ids = attention_mask.long().cumsum(-1) - 1
    position_ids.masked_fill_(attention_mask == 0, 1)

    hidden_states = inputs_embeds
    position_embeddings = rotary_emb(hidden_states, position_ids)

    # decoder layers
    all_hidden_states = {}
    all_self_attns = () 
    next_decoder_cache = None

    causal_mask = model.model._update_causal_mask(
        attention_mask, inputs_embeds, None, None, False
    )

    answer_probs = {'yes': [] , 'no': []}
    answer_logs = {'yes': [] , 'no': []}

    for l, decoder_layer in enumerate(layers):
        
        all_hidden_states[l] = hidden_states

        if l_lrp[0] is not None:
            if l_lrp[0] == l:
                hidden_states_ = hidden_states.detach().cpu().numpy().squeeze()

        layer_outputs = decoder_layer(
            hidden_states,
            attention_mask=causal_mask,
            position_ids=position_ids,
            past_key_value=None,
            output_attentions=False,
            use_cache=False,
            cache_position=None,
            position_embeddings=position_embeddings,
        )

        hidden_states = layer_outputs[0]

    hidden_states = norm(hidden_states)
    
    all_hidden_states[l+1] = hidden_states

    output_scores = lm_head(hidden_states)

    pred_intermediate = output_scores[:, -1, :][0].argmax()
    probs = nn.functional.softmax(output_scores[:,-1,:], dim=-1)
    p = probs[:,int(pred_intermediate)].squeeze().detach().cpu().numpy()

    # Collect last layer probabilities
    if l_lrp[0] == 31:
        str_ = ''
        for tok_, id_ in [('yes', yes_ids), ('no', no_ids)]:

            log1 = output_scores[:,-1,:][:,int(id_[0])].squeeze().detach().cpu().numpy()
            log2 = output_scores[:,-1,:][:,int(id_[1])].squeeze().detach().cpu().numpy()
            
            p1 = probs[:,int(id_[0])].squeeze().detach().cpu().numpy()
            p2 = probs[:,int(id_[1])].squeeze().detach().cpu().numpy()
            str_ +=  tok_ + ' {:0.3f} {:0.3f} \t'.format(p1, p2)


            answer_probs[tok_].append(np.max([p1, p2]))
            answer_logs[tok_].append(np.max([log1, log2]))


        print('final', l, int(pred_intermediate), tokenizer.convert_ids_to_tokens([int(pred_intermediate)]), p)
        print(str_)
    

    return all_hidden_states, output_scores, hidden_states_,  answer_logs, answer_probs

def decode(x):
    txt = tokenizer.decode(x.squeeze(), skip_special_tokens=False)
    return txt


def hex_to_rgb(hex_color):
    """Convert a hex color string to an RGB array normalized to [0, 1]."""
    return np.array(mcolors.to_rgb(hex_color))



def get_color_transparent(rgba_color, transparency=100):
    hex_color = "#{:02x}{:02x}{:02x}{:02x}".format(int(rgba_color[0] * 255), int(rgba_color[1] * 255), int(rgba_color[2] * 255), transparency)
    return hex_color


import re

def extract_isolated_capitals(input_string):
    # Use regex to find single capital letters surrounded by non-word characters or spaces
    return re.findall(r'\b[A-Z]\b', input_string)


from transformers import AutoTokenizer, AutoModelForCausalLM

model_name =  "meta-llama/Meta-Llama-3.1-8B-Instruct"

model_config = AutoConfig.from_pretrained(
    model_name
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    trust_remote_code=True,
    config=model_config,
    quantization_config=None,
    device_map='auto',
    torch_dtype=torch.float16)

_ = model.eval()

tokenizer = AutoTokenizer.from_pretrained(model_name)
embedding_module = model.model.embed_tokens

answer_tokens = ['yes', 'no', 'Yes', 'No', 'Ġno', 'Ġyes'] 
stop_token_ids = [tokenizer(token, add_special_tokens=False).input_ids for token in answer_tokens]

yes_ids = [9891, 9642]
no_ids = [2201, 2822]

stop_token_ids_decoded = [tokenizer.decode(token) for token in stop_token_ids]
stop_token_ids_decoded_split = [tokenizer.convert_ids_to_tokens(token) for token in stop_token_ids]


green_color = np.array([0, 68, 27])/255.
red_color = np.array([103, 0, 13])/255.

special_string = ''

sentence_list = [
                ("From the hotel lobby, there are various rooms denoted by letter names. The lobby connects to O, which connects to W and Q. The lobby also connects to G, which connects to V and M.", "W", ["W", "O", "Q"], "Yes"),
]

start_room = 'the lobby'

question = "if someone can get to {} from {}?"


request_template =    ['Given a description of a set of rooms, determine ' + question + ' \n'
            'Only answer "Yes" or "No". \n'
            'The description is: {} \n'
              'Answer: ']

transparency=95
proj_method = 'tsne'
SEED = 1


res_dir = 'algeval/test_{}'.format(start_room)
set_up_dir(res_dir)

prompts = []
  
for k, (sentence, target_room, target_branch_nodes, target_answer) in enumerate(sentence_list):

    color_dict = {'lobby': cm.Blues(0.8)} 
    
    request = request_template[0].format(target_room, start_room, sentence)

    node_names_raw = extract_isolated_capitals(sentence)

    node_names = ['Ġlobby'] + ['Ġ'+n_ for n_ in node_names_raw]
    _ = color_dict.update({n_: green_color for n_ in node_names_raw})
    
    goal_nodes = ['lobby',  target_room ]
    
    messages = [
        {
            "role": "user",
            "content": request,
        },
    ]

    prompts.append('{}) {}'.format(k+1, request))
    
    tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True,
                                               return_tensors="pt").to(model.device)

    node_dict, node_dict_names = get_node_dict(tokenized_chat, node_names)

    focus_nodes = list(itertools.chain(*node_dict.values()))

    focus_names = [node_dict_names[n_] for n_ in focus_nodes]
        
    prompt_length = tokenized_chat.shape[1]
    
    # Create a stopping criteria list    
    stopping_criteria = StoppingCriteriaList([StopOnAnyTokenSequence(stop_token_ids)])
    
    # Regenerate for explainability but with a stopping criterion, requires seed to be set again
    torch.manual_seed(SEED)
    
    # Generate with a stopping criteria and from input_ids (to find prompt length until answer)
    output_until_answer = model.generate(tokenized_chat,
                                         max_new_tokens=5,
                                         eos_token_id=tokenizer.eos_token_id,
                                         pad_token_id=tokenizer.eos_token_id,
                                         repetition_penalty= 1, #config.repetition_penalty,
                                         do_sample=False,
                                         num_return_sequences=1,
                                         stopping_criteria=stopping_criteria,
                                         use_cache=False
                                        )
    
    print(decode(output_until_answer[0]))
    
    responses = tokenizer.decode(output_until_answer[0][prompt_length:], skip_special_tokens=True)
    
    tokenized_chat_until_answer = output_until_answer[:, :-1]
    
    embeddings_out = embedding_module(tokenized_chat_until_answer)
    embeddings_ = embeddings_out.detach().requires_grad_(True)
    
    torch.manual_seed(SEED)
    output_xai = model.generate.__wrapped__(model,
                                            inputs_embeds=embeddings_,
                                            max_new_tokens=1,  # just generate one next token
                                            eos_token_id=tokenizer.eos_token_id,
                                            pad_token_id=tokenizer.eos_token_id,
                                            repetition_penalty= 1., #config.repetition_penalty,
                                            do_sample=False,
                                            num_return_sequences=1,
                                            stopping_criteria=stopping_criteria,
                                            return_dict_in_generate=True,
                                            output_scores=True,
                                            output_logits=True,
                                            use_cache=False
                                            )
    
    
    responses = tokenizer.decode(output_until_answer[0].squeeze(), skip_special_tokens=True)

    # Take the logits of the last generated token (which should be the answer token)
    xai_scores = output_xai.scores[-1]
    xai_logits = output_xai.logits[-1]
    
    xai_generated_ids = output_xai.sequences[0]
    xai_output_ids = torch.cat([tokenized_chat_until_answer, xai_generated_ids[None, :]], dim=-1)
    
    
    xai_output_ids_ = xai_output_ids[0].tolist()
    output_words = tokenizer.convert_ids_to_tokens(xai_output_ids_, skip_special_tokens=False)
    output_words = [token.replace('Ġ', '') for token in output_words]
        
    assert xai_scores.argmax() == xai_logits.argmax()
    
    next_token = xai_logits.argmax()
    token_explained = tokenizer.decode([next_token]).strip()
    
    orig_logit = xai_logits[:, next_token]
    
    
    if token_explained not in list(itertools.chain(*stop_token_ids_decoded_split)):
        print('not in list', token_explained)
        example['relevance_{}'.format(config.xai)] = 'N/T'
        compute_xai = False
        raise
    
    responses_xai = tokenizer.decode(xai_output_ids[prompt_length:], skip_special_tokens=False)
    selected_logit = xai_logits[:, next_token]       
    output_xai = model(inputs_embeds=embeddings_, output_hidden_states = True)
    hidden_states = output_xai.hidden_states

    ######### also do it for each layer  #########

    all_states = {}
    attention_mask = torch.ones_like(tokenized_chat_until_answer)
    
    start_ = 0

    all_hidden_focus_nodes = {l:{} for l in range(start_, 32)}
    
    for l in range(start_, 32):
            
        outputs, o2, hidden_states_, answer_logs, answer_probs = simplified_forward(model.model.layers, model.model.norm, model.model.rotary_emb, 
                                         model.lm_head, embeddings_, attention_mask, l_lrp=(l, next_token))

        all_states[l] =  hidden_states_

    assert l+1==32
    
    all_states[l+1] =  outputs[l+1][:,-1,:].detach().cpu().numpy().squeeze()

    contrast_token = get_token_pair(token_explained)

    contrast_id = torch.tensor(tokenizer(contrast_token, add_special_tokens=False).input_ids).cuda()
    class_id = torch.tensor(tokenizer(token_explained, add_special_tokens=False).input_ids).cuda()

    contrast_vec0 = model.lm_head.weight[contrast_id].detach().cpu().numpy().squeeze()
    class_vec0 = model.lm_head.weight[class_id].detach().cpu().numpy().squeeze()
    

    H_all = [contrast_vec0, class_vec0]
    L_all = [ contrast_token.lower(), token_explained.lower() ]
    C_all = ['pink', 'green']


    keys_ = list(range(start_, 32))

    if 31 not in keys_:
        keys_ += [31]

    all_last = []
    for k_ in  keys_:

        h = all_states[k_]

        # Focus nodes only
        h_rep = h[focus_nodes]
        
        L = [node_dict_names[n_].replace('Ġ','') for n_ in focus_nodes]
        c_transparency =  int(((k_+4)/(32+3))*100)


        assert len(L) == len(h_rep)
        for node_, hh in zip(L, h_rep):
            all_hidden_focus_nodes[k_][node_] = hh
        
        C = []

        # Determine the color for the repsective set of nodes/branch
        color_dict_all = {}
        for l_ in L:
            node_ = l_.split(' ')[0]

            ##### GOAL NODES #### (Lobby or other)
            if node_ in goal_nodes:
                if int(l_.split(' ')[1])==0:
                    c_ = get_color_transparent(cm.Purples(0.8), c_transparency)
                    C.append(c_)
                else:
                    if 'lobby' in l_:
                        c_ = get_color_transparent(cm.Blues(0.8), c_transparency)
                    else:
                        c_ = get_color_transparent(green_color, c_transparency)
                    C.append(c_)

            ##### Non-goal NODES #### (Lobby or other)
            else:
                if node_ in target_branch_nodes:
                    c_ = get_color_transparent(green_color, c_transparency )
                    C.append(c_)
                else:
                    c_ = get_color_transparent(red_color, c_transparency )
                    C.append(c_)

            color_dict_all[l_] = c_

        
        all_last.append(all_states[32])
        
    
        H_all.extend(h_rep)
        L_all.extend([l_ + ' ' + str(k_) for l_ in L])
        C_all.extend(C)

        assert len(H_all) == len(C_all)


    # adding final hidden state rep (used for determining the answer)
    H_all += [all_states[32]]
    L_all += ['eos']
    C_all += ['cyan']


    all_hidden_focus_nodes['eos'] = all_states[32]
    all_hidden_focus_nodes['yes'] = class_vec0
    all_hidden_focus_nodes['no'] = contrast_vec0
    
    ########################################################################
    
    score_class = (model.lm_head.weight[class_id]*outputs[l+1][:,-1,:]).sum().detach().cpu().numpy()
    score_contrast = (model.lm_head.weight[contrast_id]*outputs[l+1][:,-1,:]).sum().detach().cpu().numpy()


    print(class_id, score_class, contrast_id, score_contrast)
    print('*****')


    f, ax = plt.subplots(1,1, figsize=(7,5))

    reduced_embeddings = get_projection(np.array(H_all), method=proj_method, labels=L_all, ax=None, c=C_all,
                                perplexity=20, fontsize=5, s=70,
                                n_neighbors=32,
                                min_dist=.6)
    
    for i, label in enumerate(L_all):
        x, y = reduced_embeddings[i]
        ax.scatter(x, y, s=70, color= C_all[i])

        label_ = label.strip().split(' ')

        fontsize = 10

        if len(label_)>1:
            layer_idx = label_[2]

            if int(layer_idx)==31:
                label = label_[0]  #+ ' ' + label_[2]
                fontsize = 16

            elif int(layer_idx)%4 == 0:
                label =  label_[2]
            else:
                label = ''

        else:
            label = label_[0]
            fontsize = 12
           
        ax.text(x + np.random.normal(0,0.03), y + np.random.normal(0,0.03), label, fontsize=fontsize, alpha=1.)
    

    seq = {}    
    for n_ in node_dict_names.values():
        seq[n_] = []
        for i, label in enumerate(L_all):
            if n_.replace('Ġ','') in label:
                seq[n_].append( reduced_embeddings[i,:])
    
        n_name = n_.replace('Ġ','').split(' ')[0]
        try:
            c_ = color_dict_all[n_.replace('Ġ','')]   #cm.Purples(1.0) if n_name in goal_nodes else color_dict[n_name]
        except:
            import pdb;pdb.set_trace()
        
        ax.plot(np.array(seq[n_])[:,0], np.array(seq[n_])[:,1], alpha=0.5, color=c_) 
    

    ax.spines[['right', 'top']].set_visible(False)

    ax.set_xlabel(proj_method + ' 1', fontsize=14)
    ax.set_ylabel(proj_method + ' 2',  fontsize=14)

    str_ = ''
    for kk,vv in answer_probs.items():
        str_ += kk + ': {:0.3f} '.format(vv[-1])
    
    ax.set_title('{}->{} | '.format(start_room, target_room) + str_,) 

    f.tight_layout()
    f.savefig(os.path.join(res_dir, str(k) +'_' +special_string +'_' + proj_method + '.pdf'), dpi=300, transparent=True)
    plt.show()

    pickle.dump(all_hidden_focus_nodes, open(os.path.join(res_dir, str(k) + '.p'), 'wb'))

with open(os.path.join(res_dir,  "prompts.txt"), "w") as file:
    file.write("\n".join(prompts))



